{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3cd73a8-ec18-4859-8fe2-006953e50b8e",
   "metadata": {},
   "source": [
    "## Q1. What is the purpose of grid search cv in machine learning, and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439a7e38-a97f-4a11-b1ed-c7dc1ef441be",
   "metadata": {},
   "source": [
    "Grid Search assesses the performance for each possible combination of the hyperparameters and their values, chooses the combination with the best performance, and takes that combination as its starting point. With so many hyperparameters involved, processing becomes time-consuming and expensive. Cross-validation is done in GridSearchCV in addition to Grid Search. Cross-validation is employed while the model is trained to validate the outcomes against a dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1661074-9f69-40ed-a396-a91aeb2a65c4",
   "metadata": {},
   "source": [
    "In its most basic form, grid search is a method that uses brute force to estimate hyperparameters. Let’s say you have k hyperparameters, and there are ci possible values for each of them. Taking a Cartesian product of these potential values is essentially what grid search is. Although grid search may appear highly inefficient, it can be sped up using parallel processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca013007-e1c1-4bb5-a273-f1df90ee91e0",
   "metadata": {},
   "source": [
    "## Q2. Describe the difference between grid search cv and randomize search cv, and when might you chooseone over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7952c1-c14c-4cf0-8da8-ce25719a2795",
   "metadata": {},
   "source": [
    "Grid search is a method for hyperparameter optimization that involves specifying a list of values for each hyperparameter that you want to optimize, and then training a model for each combination of these values. For example, if you want to optimize two hyperparameters, alpha and beta, with grid search, you would specify a list of values for alpha and a separate list of values for the beta. The grid search algorithm would then train a model using every combination of these values and evaluate the performance of each model. The optimal values for the hyperparameters are then chosen based on the performance of the models.\n",
    "\n",
    "Additionally, it is recommended to use cross-validation when performing hyperparameter optimization with either grid search or randomized search. Cross-validation is a technique that involves splitting the training data into multiple sets and training the model multiple times, each time using a different subset of the data as the validation set. This can provide a more accurate estimate of the model’s performance and help to avoid overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16176aa-0b26-4433-ab78-eeb84dcd55cd",
   "metadata": {},
   "source": [
    "Randomized search is another method for hyperparameter optimization that can be more efficient than grid search in some cases. With randomized search, instead of specifying a list of values for each hyperparameter, you specify a distribution for each hyperparameter. The randomized search algorithm will then sample values for each hyperparameter from its corresponding distribution and train a model using the sampled values. This process is repeated a specified number of times, and the optimal values for the hyperparameters are chosen based on the performance of the models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af78078-512d-4432-a28c-6489d56d0493",
   "metadata": {},
   "source": [
    "RandomizedSearchCV can be more efficient if the search space is large since it only samples a subset of the possible combinations rather than evaluating them all. This can be especially useful if the model is computationally expensive to fit, or if the hyperparameters have continuous values rather than discrete ones. In these cases, it may not be feasible to explore the entire search space using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe6f10a-93a9-42af-abe3-a0682c8fdc7d",
   "metadata": {},
   "source": [
    "## Q3. What is data leakage, and why is it a problem in machine learning? Provide an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f60302-13f1-4664-9933-fee651f17ba6",
   "metadata": {},
   "source": [
    "If there is a high correlation between the target variable and the input features then this situation is referred to as data leakage. This is because when we train our model with that highly correlated feature then the model gets most of the target variable’s information in the training process only and it has to do very little to achieve high accuracy. In this situation, the model gives pretty decent performance both on the training as well as the validation data but as we use that model to make actual predictions then the model’s performance is not up to the mark. This is how we can identify data leakage.\n",
    "\n",
    "Data transmitted via emails, API calls, chat rooms, and other communications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7d45e8-42c8-491e-a851-d2ca914333e2",
   "metadata": {},
   "source": [
    "## Q4. How can you prevent data leakage when building a machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4904ea3b-ed76-4442-a984-48df88d04a64",
   "metadata": {},
   "source": [
    "Here are some methods for detecting and preventing data leakage in machine learning:\n",
    "\n",
    "Feature engineering: Feature engineering is the process of selecting and transforming input features to improve the performance of machine learning models. When designing features, it is important to consider which features are likely to cause data leakage. For example, if a model is being trained to predict customer churn, using information about whether a customer has already churned can lead to data leakage. In general, it is best to avoid using features that are directly related to the target variable.\n",
    "\n",
    "Proper data splitting: Proper data splitting is essential for preventing data leakage. The most common approach is to split the data into training, validation, and test sets. The training set is used to train the model, the validation set is used to tune hyperparameters and evaluate the model during training, and the test set is used to evaluate the model’s generalization performance after training. It is important to ensure that there is no overlap between the data in the training, validation, and test sets to prevent data leakage.\n",
    "\n",
    "Cross-validation: Cross-validation is a technique for evaluating the performance of machine learning models that involves repeatedly splitting the data into training and validation sets. This can help detect data leakage by revealing if the model is overfitting to specific subsets of the data. It is important to ensure that the data is properly shuffled before applying cross-validation to prevent leakage.\n",
    "\n",
    "Proper data preprocessing: Data preprocessing, such as normalization or scaling, can inadvertently leak information about the test set into the training set. It is important to ensure that the preprocessing steps are based only on the training set and not on the test set.\n",
    "\n",
    "Regularization: Regularization is a technique for reducing overfitting in machine learning models. By adding a penalty term to the loss function, the model is encouraged to learn simpler patterns that are more likely to generalize to new data. Regularization can be effective at preventing data leakage by reducing the model’s reliance on specific features or subsets of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cbfe94-7f92-48ed-bc5b-cfb5f2c35ea9",
   "metadata": {},
   "source": [
    "### Q5. What is a confusion matrix, and what does it tell you about the performance of a classification model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc3cf98a-cbb3-4607-a5ce-461f31e55360",
   "metadata": {},
   "source": [
    "A confusion matrix is a matrix that summarizes the performance of a machine learning model on a set of test data. It is often used to measure the performance of classification models, which aim to predict a categorical label for each input instance. The matrix displays the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) produced by the model on the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0ef74d-d1d2-4b4f-b9d8-b99437e26395",
   "metadata": {},
   "source": [
    "It evaluates the performance of the classification models, when they make predictions on test data, and tells how good our classification model is.\n",
    "\n",
    "It not only tells the error made by the classifiers but also the type of errors such as it is either type-I or type-II error.\n",
    "\n",
    "With the help of the confusion matrix, we can calculate the different parameters for the model, such as accuracy, precision, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3d1170-a83e-4a7d-8688-d908bffdbf71",
   "metadata": {},
   "source": [
    "## Q6. Explain the difference between precision and recall in the context of a confusion matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd1388d-9ae9-404f-a2cc-16f09f725b4e",
   "metadata": {},
   "source": [
    "Precision is the ratio of true positives to the total of the true positives and false positives. Precision looks to see how much junk positives got thrown in the mix. If there are no bad positives (those FPs), then the model had 100% precision. The more FPs that get into the mix, the uglier that precision is going to look.To calculate a model’s precision, we need the positive and negative numbers from the confusion matrix.\n",
    "\n",
    "Precision = TP/(TP + FP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a43958d-7e9a-4804-9cda-a7d9d8f85478",
   "metadata": {},
   "source": [
    "Recall goes another route. Instead of looking at the number of false positives the model predicted, recall looks at the number of false negatives that were thrown into the prediction mix.\n",
    "\n",
    "Recall = TP/(TP + FN)\n",
    "\n",
    "The recall rate is penalized whenever a false negative is predicted. Because the penalties in precision and recall are opposites, so too are the equations themselves. Precision and recall are the yin and yang of assessing the confusion matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41bbd98-1cd6-4492-9c55-9ef8f2fa6bb8",
   "metadata": {},
   "source": [
    "when understanding the confusion matrix, sometimes a model might want to allow for more false negatives to slip by. That would result in higher precision because false negatives don’t penalize the recall equation. (There, they’re a virtue.)\n",
    "\n",
    "Sometimes a model might want to allow for more false positives to slip by, resulting in higher recall, because false positives are not accounted for.\n",
    "\n",
    "Generally, a model cannot have both high recall and high precision. There is a cost associated with getting higher points in recall or precision. A model may have an equilibrium point where the two, precision and recall, are the same, but when the model gets tweaked to squeeze a few more percentage points on its precision, that will likely lower the recall rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e0bd3c-9579-4372-87a8-4969824ade96",
   "metadata": {},
   "source": [
    "## Q7. How can you interpret a confusion matrix to determine which types of errors your model is making?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05bf0fb-a0d8-43ba-87a4-35f6f07ad774",
   "metadata": {},
   "source": [
    "False Positive Rate (caused by Type I Error): \n",
    "\n",
    "tells us how often the model predicts ‘yes’ for an actual ‘no’. Is it important to keep this error low? It may be a yes or a no and depends on the scenario as illustrated below:\n",
    "\n",
    "Sometimes, this error might translate to a simple case where a person is predicted to have some bacterial infection while actually that might not be the case. The medication to treat simple bacterial infections might not be very dangerous and is believed to have very mild or no side effects on the patient. So, in such cases, we might not worry much about the Type I error.\n",
    "\n",
    "But things can get complicated and serious if the same error happens in a scenario where a person not suffering from cancer is diagnosed to have cancer. This can be really dangerous and sometimes fatal due to the high doses of radiation and chemotherapy that a patient can be exposed to.\n",
    "\n",
    "True Negative Rate (or Specificity)is a metric that tells us how often the model predicts ‘no’ for an actual ‘no’. It is equivalent to 1 minus False Positive Rate.\n",
    "\n",
    "False Negative Rate (caused by Type II Error):\n",
    "\n",
    " Number of items the model wrongly predicted ‘no’ out of the total actual ‘yes’. This metric is especially important in most binary classification problems, as it tells us the frequency with which a positive instance is wrongly identified as negative. For example, if a cancer patient is wrongly diagnosed as not having cancer, that individual would either go undiagnosed or misdiagnosed. Similarly, identifying a fraudulent transaction as non-fraudulent can cause several serious repercussions for a bank. Hence, whenever we intend our model to be a diagnostic aid, we would always want this metric to be as low as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1de428c-6a27-40c5-a6b1-4237b9f57aa9",
   "metadata": {},
   "source": [
    "## Q8. What are some common metrics that can be derived from a confusion matrix, and how are they calculated?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b37231-94f1-40c9-8f81-18c0918fdf51",
   "metadata": {},
   "source": [
    "Accuracy:  Accuracy is used to measure the performance of the model. It is the ratio of Total correct instances to the total instances. \n",
    "\n",
    "Accuracy =  {TP+TN}{TP+TN+FP+FN}\n",
    "\n",
    "For the above case:\n",
    "\n",
    "Accuracy = (5+3)/(5+3+1+1) = 8/10 = 0.8\n",
    "\n",
    "Precision: Precision is a measure of how accurate a model’s positive predictions are. It is defined as the ratio of true positive predictions to the total number of positive predictions made by the model\n",
    "\n",
    "{Precision} = {TP}/{TP+FP}\n",
    "For the above case:\n",
    "\n",
    "Precision = 5/(5+1) =5/6 = 0.8333\n",
    "\n",
    "Recall: Recall measures the effectiveness of a classification model in identifying all relevant instances from a dataset. It is the ratio of the number of true positive (TP) instances to the sum of true positive and false negative (FN) instances.\n",
    "\n",
    "{Recall} = {TP}{TP+FN}\n",
    "\n",
    " For the above case:\n",
    "\n",
    "Recall = 5/(5+1) =5/6 = 0.8333\n",
    "\n",
    "F1-Score: F1-score is used to evaluate the overall performance of a classification model. It is the harmonic mean of precision and recall,\n",
    "\n",
    "{F1-Score} =  {2  Precision*Recall}/{Precision + Recall}\n",
    "\n",
    " For the above case:\n",
    "\n",
    "F1-Score: = (2* 0.8333* 0.8333)/( 0.8333+ 0.8333)  = 0.8333\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabacfee-ee9f-4fcb-8dd7-f70f3a8c03ad",
   "metadata": {},
   "source": [
    "## Q9. What is the relationship between the accuracy of a model and the values in its confusion matrix?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de2d47a-04c6-459b-b1d1-60fd42ad0db1",
   "metadata": {},
   "source": [
    "AI accuracy is the percentage of correct classifications that a trained machine learning model achieves, i.e., the number of correct predictions divided by the total number of predictions across all classes. It is often abbreviated as ACC.\n",
    "\n",
    "ACC is reported as a value between [0,1] or [0, 100], depending on the chosen scale. Accuracy of 0 means the classifier always predicts the wrong label, whereas accuracy of 1, or 100, means that it always predicts the correct label. \n",
    "\n",
    "A nice characteristic of this metric is that it has a direct relationship with all values of the confusion matrix. These are the four pillars of supervised machine learning evaluation: true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "Starting from the confusion matrix, we can see this relationship by deriving the statistical formula for accuracy. Note that we do so on binary classification for simplicity, but the same concept can be easily extended to more than two classes.\n",
    "\n",
    "Accuracy is a proportional measure of the number of correct predictions over all predictions.\n",
    "Correct predictions are composed of true positives (TP) and true negatives (TN).\n",
    "All predictions are composed of the entirety of positive (P) and negative (N) examples.\n",
    "P is composed of TP and false positives (FP), and N is composed of TN and false negatives (FN). \n",
    "\n",
    "Thus, we can define accuracy as ACC =TP + TNTP + TN + FN + TP =TP + TNP + N.\n",
    "\n",
    "It is important to also emphasize that evaluating model accuracy should be done on a statistically significant number of predictions as per any metric evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b1f186-7220-4ff8-b49f-fff179af5efb",
   "metadata": {},
   "source": [
    "### Q10. How can you use a confusion matrix to identify potential biases or limitations in your machine learning model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e144af-63d3-421a-9ee2-baa8621b7b0d",
   "metadata": {},
   "source": [
    "Confusion matrix is a popular way of understanding how a classifier is doing, in terms of the true positives, false positives, true negatives and the false negatives. Here are some popular metrics that can be compared across various groups in question to measure fairness based on the confusion matrix:\n",
    "\n",
    "Equal Opportunity: Is the True Positive Rate/Recall same across different groups ?\n",
    "Recall that TPR indicates, of all positives, how many items we actually detected as positive. The formula for TPR is :\n",
    "\n",
    "TPR = TP/ (TP+FN)\n",
    "\n",
    "Equalized Odds: Is the TPR and FPR same across different groups ? In addition to TPR, this metric looks at the False Positive Rate (FPR) across groups. Recall that the FPR denotes, Out of all negatives, how many were falsely classified as positive.\n",
    "\n",
    "    FPR = FP/ (FP + TN) = False Positives / Total Number of Negatives\n",
    "\n",
    "Accuracy: Accuracy is the fraction of correctly classified examples. It is infact the most popular classification metric. A way of measuring fairness is if the Accuracy similar across different groups.\n",
    "\n",
    "   Accuracy = (TP + TN) /  (TP + TN + FP + FN) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8f024a-8bb4-47ef-9472-d63e471e2f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
