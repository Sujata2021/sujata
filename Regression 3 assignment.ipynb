{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a9c02d1-7f36-425c-9507-d24b22fbd73c",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9228ac6b-7dd8-4a1d-8ba1-ffa0bd4472c9",
   "metadata": {},
   "source": [
    "Ridge Regression is a technique used in linear regression to address the problem of overfitting. It does this by adding a regularization term to the loss function, which shrinks the coefficients toward zero. This reduces the variance of the model and can improve its predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3234238a-e84a-4d25-b88c-da4a8403ec99",
   "metadata": {},
   "source": [
    "OLS is a traditional method that finds the line of best fit through the data by minimizing the sum of the squared errors between the predicted and actual values. However, OLS can suffer from high variance and overfitting when the number of predictor variables is large. To address this issue, Ridge Regression introduces a regularization term that shrinks the coefficients toward zero, which can lead to a better model with lower variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c917837-7093-433f-bf04-e8bf97eac37b",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f86007-cd49-48d4-baed-c5cd969e9f9a",
   "metadata": {},
   "source": [
    "Linearity: Ridge regression assumes that the relationship between the independent and dependent variables is linear. It means that the effect of each independent variable on the dependent variable is constant and additive. It is important to verify this assumption by examining scatter and residual plots to ensure that the data exhibits a linear pattern.\n",
    "\n",
    "Independence: It considers that the observations in the dataset are independent of each other. In other words, the values of the dependent variable for one observation should not be influenced by the values of the dependent variable for other observations. To satisfy this assumption, the data should be collected using random sampling or experimental designs that minimize dependencies between observations.\n",
    "\n",
    "Homoscedasticity: This regression assumes that the error terms (residuals) variance is constant across all levels of the independent variables. This is known as homoscedasticity. Violations of this assumption can result in heteroscedasticity, where the residual spread differs for different independent variable values. To assess homoscedasticity, residual plots can be examined, and statistical tests such as the Breusch-Pagan test can be conducted.\n",
    "\n",
    "No Multicollinearity: Ridge regression assumes no perfect multicollinearity among the independent variables. Perfect multicollinearity occurs when two or more independent variables are perfectly linearly related, making it impossible to estimate their individual effects accurately. Ridge regression helps address this assumption by shrinking the coefficients, but it is still important to check for multicollinearity using methods like variance inflation factor (VIF) analysis.\n",
    "\n",
    "Normally Distributed Errors: It assumes that the errors (residuals) follow a normal distribution with a zero mean. This assumption ensures the validity of statistical inference and hypothesis testing. Checking the normality of the residuals can be done through a visual examination of a histogram or by conducting formal tests like the Shapiro-Wilk test.\n",
    "\n",
    "No Endogeneity: Ridge regression believes there is no endogeneity, which occurs when there is a correlation between the independent variables and the error term. Endogeneity can lead to biased coefficient estimates and invalid statistical inferences. Techniques like instrumental variable regression can be employed to address endogeneity if it is suspected."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91ca89e-2b7b-4298-855c-466dc28b3e79",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ab2d3-d06e-42e6-85ea-d4b7b3180fc8",
   "metadata": {},
   "source": [
    "When λ = 0, no parameters are eliminated. The estimate is equal to the one found with linear regression.\n",
    "\n",
    "As λ increases, more and more coefficients are set to zero and eliminated.\n",
    "\n",
    "When λ = ∞, all coefficients are eliminated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b45b38b-0cca-4c20-9fe4-70d5aaea1112",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fb7bd2-6074-471a-b89c-1798b46cac2e",
   "metadata": {},
   "source": [
    "NO,The ridge penalty shrinks the regression coefficient estimate towards zero, but not exactly zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d78ecf0-a9e5-4dd0-b22a-55d587bfb7f0",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ba330-a3fa-410c-ae43-24273116f8a3",
   "metadata": {},
   "source": [
    "Multicollinearity can create inaccurate estimates of the regression coefficients, inflate the standard errors of the regression coefficients, deflate the partial t-tests for the regression coefficients, give false, nonsignificant, p-values, and degrade the predictability of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8acc61-e541-4faf-9b85-1768ebcca324",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101954f6-877d-4df7-a38b-83788afcb237",
   "metadata": {},
   "source": [
    "YES ,by using hot encoding technique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46443c3a-af66-4e87-97da-5ab3e8ca93ae",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f78dabff-0fa2-4c36-93f6-5820c3bf3cd5",
   "metadata": {},
   "source": [
    "A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e35ed2a4-7246-4010-a4ba-4d9e087af851",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598e965-a508-42b6-8092-233b95aa9ebe",
   "metadata": {},
   "source": [
    "The ridge regression technique can be used to predict time-series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7629596-bda3-47b3-b2c9-cb4da554e5ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
