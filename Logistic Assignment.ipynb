{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "647f0a63-de8b-49bf-96dd-eee1e8e883d0",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between linear regression and logistic regression models. Provide an example ofa scenario where logistic regression would be more appropriate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6896a769-84ac-4085-afd2-62a51bfabf94",
   "metadata": {},
   "source": [
    "Linear Regression is a supervised regression model.\n",
    "\n",
    "Logistic Regression is a supervised classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695b69c4-6f4f-4ad0-aa26-b55a297d3bf4",
   "metadata": {},
   "source": [
    "In Linear Regression, we predict the value by an integer number.\n",
    "\n",
    "In Logistic Regression, we predict the value by 1 or 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c10da4-5de1-419d-97dc-7e2326a4867c",
   "metadata": {},
   "source": [
    "In linear Regression dependent variable should be numeric and the response variable is continuous to value.\t\n",
    "\n",
    "In the logistic regression the dependent variable consists of only two categories. Logistic regression estimates the odds outcome of the dependent variable given a set of quantitative or categorical independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd35e9d-3835-41d7-9c27-c76313307492",
   "metadata": {},
   "source": [
    "Applications of linear regression:\n",
    "\n",
    "Financial risk assessment\n",
    "\n",
    "Business insights\n",
    "\n",
    "Market analysis\n",
    "\n",
    "\n",
    "Applications of logistic regression:\n",
    "\n",
    "Medicine\n",
    "\n",
    "Credit scoring\n",
    "\n",
    "Hotel Booking\n",
    "\n",
    "Gaming\n",
    "\n",
    "Text editing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36e97a6-a1ac-4672-8a90-dc01cf5934bb",
   "metadata": {},
   "source": [
    "## Q2. What is the cost function used in logistic regression, and how is it optimized?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f0d742-b716-49e1-97de-d34b3015cb7e",
   "metadata": {},
   "source": [
    "For Logistic Regression,\n",
    "\n",
    "h_{\\Theta}(x) = g(\\Theta^{T}x) \n",
    "\n",
    "It will result in a non-convex cost function as shown above. So, for Logistic Regression the cost function we use is also known as the cross entropy or the log loss.\n",
    "\n",
    "Cost(h_{Theta}(x),y) = } -log(h_{Theta}(x))  ify=1\n",
    "\n",
    "                          -log(1-h_{Theta}(x)) if y = 0 \n",
    "\n",
    "Case 1: If y = 1, that is the true label of the class is 1. Cost = 0 if the predicted value of the label is 1 as well. But as hθ(x) deviates from 1 and approaches 0 cost function increases exponentially and tends to infinity which can be appreciated from the below graph as well.\n",
    "\n",
    "Case 2: If y = 0, that is the true label of the class is 0. Cost = 0 if the predicted value of the label is 0 as well. But as hθ(x) deviates from 0 and approaches 1 cost function increases exponentially and tends to infinity which can be appreciated from the below graph as well.\n",
    "\n",
    "With the modification of the cost function, we have achieved a loss function that penalizes the model weights more and more as the predicted value of the label deviates more and more from the actual label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96edde6f-ff13-47a3-9bc4-40a6c57ad5ce",
   "metadata": {},
   "source": [
    "## Q3. Explain the concept of regularization in logistic regression and how it helps prevent overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0657d4-755b-423b-b991-9d0378842876",
   "metadata": {},
   "source": [
    "Regularization is a technique used to avoid overfitting by adding a penalty term to the objective function (loss function) that the model is trying to minimize. This penalty term reduces the complexity of the model and improves its generalization by reducing the variance of the model.\n",
    "\n",
    "There are several types of regularization that can be used in logistic regression, including L1 (Lasso), L2 (Ridge), and Elastic Net. These types differ in the form of the penalty term and the types of coefficients that they regularize.\n",
    "\n",
    "To choose the regularization strength (hyperparameter), you can use cross-validation to select the hyperparameter that gives the best performance on the validation set, try a range of values (e.g. 10^-6 to 10⁶) and use a logarithmic scale, or use a grid search to try a range of values for multiple hyperparameters at once.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7197d5-d082-4d31-b062-4375160f50a2",
   "metadata": {},
   "source": [
    "## Q4. What is the ROC curve, and how is it used to evaluate the performance of the logistic regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce81c7bb-0e7b-4c15-9f17-07fa59b36516",
   "metadata": {},
   "source": [
    "ROC or Receiver Operating Characteristic plot is used to visualise the performance of a binary classifier. It gives us the trade-off between the True Positive Rate (TPR) and the False Positive Rate (FPR) at different classification thresholds."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e824e37-70ba-4155-b54e-ee5ab3ba6dc6",
   "metadata": {},
   "source": [
    "True Positive Rate:\n",
    "\n",
    "True Positive Rate is the proportion of observations that are correctly predicted to be positive.\n",
    "\n",
    "False Positive Rate:\n",
    "False Positive Rate is the proportion of observations that are incorrectly predicted to be positive.\n",
    "\n",
    "\n",
    "For different threshold values we will get different TPR and FPR. So, in order to visualise which threshold is best suited for the classifier we plot the ROC curve. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac0a027-c08a-406c-a19f-6121913bfc29",
   "metadata": {},
   "source": [
    "The TPR and FPR are shown on the ROC curve for various categorization criteria. A perfect classifier would properly identify all positive situations and make no false positive predictions, resulting in a TPR of 1 and an FPR of 0.\n",
    "\n",
    "A typical statistic for assessing the effectiveness of a binary classification model is the area under the ROC curve (AUC). Better model performance is indicated by a higher AUC value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1222536-0255-4fa0-ae15-365af99ec65e",
   "metadata": {},
   "source": [
    "## Q5. What are some common techniques for feature selection in logistic regression? How do thesetechniques help improve the model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a484dd-2d6f-4971-8159-607dfcdbe4a5",
   "metadata": {},
   "source": [
    "There are several techniques that can be used to select the best features for a logistic regression model. One common method is to use a method called “RFE”\n",
    "\n",
    "Recursive Feature Elimination (RFE) is a feature selection technique that involves training a model on a subset of the features, and then iteratively removing the least important features one by one until we are left with the desired number of features.\n",
    "\n",
    "The idea behind RFE is to determine the contribution of each feature to the model by measuring how well the model performs when that feature is removed. The features that are most important to the model will have the greatest impact on performance when they are removed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda00936-8e69-4f48-99ff-6ffbe081df9b",
   "metadata": {},
   "source": [
    "Another method is to use a wrapper approach\n",
    "\n",
    "Wrapper approach to feature selection involves using a search algorithm to find the best combination of features for a predictive model. The goal is to find the combination of features that results in the best model performance, as measured by a metric such as accuracy or F1 score.\n",
    "\n",
    "There are several types of search algorithms that can be used for wrapper feature selection, including forward selection, backward elimination, and genetic algorithms.\n",
    "\n",
    "One popular method is “forward selection”, which involves starting with an empty set of features and iteratively adding one feature at a time until the desired number of features is reached. At each step, the model is trained and evaluated, and the feature that results in the greatest improvement in performance is added to the set.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee051fb-e2e6-4747-8f7d-2c83a770237c",
   "metadata": {},
   "source": [
    "There are also “filter” methods that can be used to select features based on statistical tests or heuristics.\n",
    "\n",
    "Filter approach to feature selection involves using statistical tests or heuristics to select the most relevant features for a predictive model. The goal is to select features that are highly correlated with the target variable, as these are likely to be the most useful for prediction.\n",
    "\n",
    "There are several types of statistical tests that can be used for filter feature selection, including chi-square, ANOVA, and mutual information. These tests measure the degree of association between the features and the target variable, and can help identify the most relevant features for the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad533572-585d-4003-bade-1ed5ba7754b3",
   "metadata": {},
   "source": [
    "## Q6. How can you handle imbalanced datasets in logistic regression? What are some strategies for dealingwith class imbalance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116ff5fe-80f7-4978-a806-98a556649013",
   "metadata": {},
   "source": [
    "1. Use the right evaluation metrics\n",
    " \n",
    "\n",
    "Applying inappropriate evaluation metrics for model generated using imbalanced data can be dangerous. Imagine our training data is the one illustrated in graph above. If accuracy is used to measure the goodness of a model, a model which classifies all testing samples into “0” will have an excellent accuracy (99.8%), but obviously, this model won’t provide any valuable information for us.\n",
    "\n",
    "In this case, other alternative evaluation metrics can be applied such as:\n",
    "\n",
    "Precision/Specificity: how many selected instances are relevant.\n",
    "Recall/Sensitivity: how many relevant instances are selected.\n",
    "F1 score: harmonic mean of precision and recall.\n",
    "MCC: correlation coefficient between the observed and predicted binary classifications.\n",
    "AUC: relation between true-positive rate and false positive rate.\n",
    " \n",
    "\n",
    "2. Resample the training set\n",
    " \n",
    "Apart from using different evaluation criteria, one can also work on getting different dataset. Two approaches to make a balanced dataset out of an imbalanced one are under-sampling and over-sampling.\n",
    "\n",
    " \n",
    "\n",
    "2.1. Under-sampling\n",
    " \n",
    "\n",
    "Under-sampling balances the dataset by reducing the size of the abundant class. This method is used when quantity of data is sufficient. By keeping all samples in the rare class and randomly selecting an equal number of samples in the abundant class, a balanced new dataset can be retrieved for further modelling.\n",
    "\n",
    " \n",
    "\n",
    "2.2. Over-sampling\n",
    " \n",
    "\n",
    "On the contrary, oversampling is used when the quantity of data is insufficient. It tries to balance dataset by increasing the size of rare samples. Rather than getting rid of abundant samples, new rare samples are generated by using e.g. repetition, bootstrapping or SMOTE (Synthetic Minority Over-Sampling Technique) [1].\n",
    "\n",
    "Note that there is no absolute advantage of one resampling method over another. Application of these two methods depends on the use case it applies to and the dataset itself. A combination of over- and under-sampling is often successful as well.\n",
    "\n",
    " \n",
    "\n",
    "3. Use K-fold Cross-Validation in the Right Way\n",
    " \n",
    "\n",
    "It is noteworthy that cross-validation should be applied properly while using over-sampling method to address imbalance problems.\n",
    "\n",
    "Keep in mind that over-sampling takes observed rare samples and applies bootstrapping to generate new random data based on a distribution function. If cross-validation is applied after over-sampling, basically what we are doing is overfitting our model to a specific artificial bootstrapping result. That is why cross-validation should always be done before over-sampling the data, just as how feature selection should be implemented. Only by resampling the data repeatedly, randomness can be introduced into the dataset to make sure that there won’t be an overfitting problem.\n",
    "\n",
    " \n",
    "\n",
    "4. Ensemble Different Resampled Datasets\n",
    " \n",
    "\n",
    "The easiest way to successfully generalize a model is by using more data. The problem is that out-of-the-box classifiers like logistic regression or random forest tend to generalize by discarding the rare class. One easy best practice is building n models that use all the samples of the rare class and n-differing samples of the abundant class. Given that you want to ensemble 10 models, you would keep e.g. the 1.000 cases of the rare class and randomly sample 10.000 cases of the abundant class. Then you just split the 10.000 cases in 10 chunks and train 10 different models.\n",
    "This approach is simple and perfectly horizontally scalable if you have a lot of data, since you can just train and run your models on different cluster nodes. Ensemble models also tend to generalize better, which makes this approach easy to handle.\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae6136e-3619-4083-8a59-8931dcad5fea",
   "metadata": {},
   "source": [
    "## Q7. Can you discuss some common issues and challenges that may arise when implementing logisticregression, and how they can be addressed? For example, what can be done if there is multicollinearityamong the independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04818e2-3cc0-492a-a955-bb14c6b05095",
   "metadata": {},
   "source": [
    "1. The dependent/response variable is binary or dichotomous\n",
    "\n",
    "The first assumption of logistic regression is that response variables can only take on two possible outcomes – pass/fail, male/female, and malignant/benign.\n",
    "\n",
    "This assumption can be checked by simply counting the unique outcomes of the dependent variable. If more than two possible outcomes surface, then one can consider that this assumption is violated.\n",
    "\n",
    "2. Little or no multicollinearity between the predictor/explanatory variables\n",
    "\n",
    "This assumption implies that the predictor variables (or the independent variables) should be independent of each other. Multicollinearity relates to two or more highly correlated independent variables. Such variables do not provide unique information in the regression model and lead to wrongful interpretation.\n",
    "\n",
    "The assumption can be verified with the variance inflation factor (VIF), which determines the correlation strength between the independent variables in a regression model.\n",
    "\n",
    "3. Linear relationship of independent variables to log odds\n",
    "\n",
    "Log odds refer to the ways of expressing probabilities. Log odds are different from probabilities. Odds refer to the ratio of success to failure, while probability refers to the ratio of success to everything that can occur.\n",
    "\n",
    "For example, consider that you play twelve tennis games with your friend. Here, the odds of you winning are 5 to 7 (or 5/7), while the probability of you winning is 5 to 12 (as the total games played = 12).\n",
    "\n",
    "4. Prefers large sample size\n",
    "Logistic regression analysis yields reliable, robust, and valid results when a larger sample size of the dataset is considered.\n",
    "\n",
    "This assumption can be validated by taking into account a minimum of 10 cases considering the least frequent outcome for each estimator variable. Let’s consider a case where you have three predictor variables, and the probability of the least frequent outcome is 0.30. Here, the sample size would be (10*3) / 0.30 = 100.\n",
    "\n",
    "5. Problem with extreme outliers\n",
    "Another critical assumption of logistic regression is the requirement of no extreme outliers in the dataset.\n",
    "\n",
    "This assumption can be verified by calculating Cook’s distance (Di) for each observation to identify influential data points that may negatively affect the regression model. In situations when outliers exist, one can implement the following solutions: \n",
    "\n",
    "Eliminate or remove the outliers\n",
    "Consider a value of mean or median instead of outliers, or\n",
    "Keep the outliers in the model but maintain a record of them while reporting the regression results\n",
    "6. Consider independent observations\n",
    "This assumption states that the dataset observations should be independent of each other. The observations should not be related to each other or emerge from repeated measurements of the same individual type.\n",
    "\n",
    "The assumption can be verified by plotting residuals against time, which signifies the order of observations. The plot helps in determining the presence or absence of a random pattern. If a random pattern is present or detected, this assumption may be considered violated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5be51c-bd5f-46c4-a564-53ae7e3fa807",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
