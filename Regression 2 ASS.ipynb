{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4944acc6-52d8-4446-a62e-6c825d1fac7a",
   "metadata": {},
   "source": [
    " ## Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056ed3f8-a040-4d46-b415-810532b75fc6",
   "metadata": {},
   "source": [
    "R-Squared (R² or the coefficient of determination) is a statistical measure in a regression model that determines the proportion of variance in the dependent variable that can be explained by the independent variable. In other words, r-squared shows how well the data fit the regression model (the goodness of fit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ab1ee1-d677-4c79-93e8-e5a1fb9deb44",
   "metadata": {},
   "source": [
    "R-squared also known as the coefficient of determination measures the variability in the dependent variable Y that is being explained by the independent variables Xi in the regression model. \n",
    "\n",
    "We calculate R-Square in the following steps\n",
    "\n",
    " First, calculate the mean of the target/dependent variable y and we denote it by y̅\n",
    " \n",
    " Calculate the total sum of squares by subtracting each observation yi from y̅, then squaring it and summing these square differences across all the values. It is denoted by  SStot = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
    " \n",
    "We estimate the model parameter using a suitable regression model such as Linear Regression or SVM Regressor  \n",
    "\n",
    "We calculate the Sum of squares due to regression which is denoted by SSR. This is calculated by subtracting each predicted value of y denoted by y_predi from yi squaring these differences and then summing all the n terms. SSR = \\sum_{i=1}^{n} (\\hat{ypred}_i – \\bar{y})^2  \n",
    "\n",
    "We calculate the sum of squares (SSres). It explains unaccounted variability in the dependent y after predicting these values from an independent variable in the model.  SSres = \\sum_{i=1}^n (y_i – {ypred}_i)^2\n",
    "we can then use either R^2 = {SSR}/{SStot}  or R^2 = 1 - {SSres}/{SStot}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9007d068-5a3c-4554-983f-66b752ce5e92",
   "metadata": {},
   "source": [
    "The goodness of fit of regression models can be analyzed on the basis of the R-square method. The more the value of the r-square near 1, the better the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e884eda-8692-4973-adb0-a2b59abc7935",
   "metadata": {},
   "source": [
    "## Q2. Define adjusted R-squared and explain how it differs from the regular R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370fa7a9-329a-4915-8f90-f3c481790d2d",
   "metadata": {},
   "source": [
    "Adjusted R-Squared is an updated version of R-squared which takes account of the number of independent variables while calculating R-squared. The main problem with R-squared is that the R-Square value always increases with an increase in independent variables irrespective of the fact that where the independent variable is contributing to the model or not. This leads to the model having high variance if the model has a lot of independent variables.\n",
    "\n",
    "Formula For Adjusted R-Squared\n",
    "\n",
    "Adjusted R^2 = 1 - {(1-R^2) * (n-1)}/{n-k-1}\n",
    "\n",
    "Here R-square is the value that we calculate using the method explained above \n",
    "n is the total number of observations in the data \n",
    "k is the number of independent variables (predictors) in the regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd013d2-acbe-43b4-abb2-7219ce672949",
   "metadata": {},
   "source": [
    "## Q3. When is it more appropriate to use adjusted R-squared?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8dd9017-c0a8-4792-ac14-68931ae5f019",
   "metadata": {},
   "source": [
    "It is better to use Adjusted R-squared when there are multiple variables in the regression model. This would allow us to compare models with differing numbers of independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632c4ff3-38ae-48da-9466-6ab835f5ba99",
   "metadata": {},
   "source": [
    "## Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics calculated, and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8751f557-ada0-46b6-a099-27d43f6cd34d",
   "metadata": {},
   "source": [
    "MAE (Mean absolute error) represents the difference between the original and predicted values extracted by averaged the absolute difference over the data set.\n",
    "\n",
    "MSE (Mean Squared Error) represents the difference between the original and predicted values extracted by squared the average difference over the data set.\n",
    "\n",
    "RMSE (Root Mean Squared Error) is the error rate by the square root of MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85897005-e24a-40ec-a655-81ec0749ee2a",
   "metadata": {},
   "source": [
    "MAE evaluates the absolute distance of the observations (the entries of the dataset) to the predictions on a regression, taking the average over all observations. We use the absolute value of the distances so that negative errors are accounted properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8640c5-b934-471a-8899-06ded8eb8056",
   "metadata": {},
   "source": [
    "By squaring the distance, so that the results are positive. This is done by the MSE, and higher errors (or distances) weigh more in the metric than lower ones, due to the nature of the power function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db474fd5-67e9-415f-b384-5244bb7d288b",
   "metadata": {},
   "source": [
    "RMSE is used then to return the MSE error to the original unit by taking the square root of it, while maintaining the property of penalizing higher errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fcbf4b-023a-4a93-b42f-3ad7c9413161",
   "metadata": {},
   "source": [
    "## Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922f7c6a-3a98-4271-82d4-1745f1c9dd07",
   "metadata": {},
   "source": [
    "MAE\n",
    "\n",
    "Advantages\n",
    "\n",
    "1.The Mean Absolute Error obtained is in a similar unit as the output variable.\n",
    "\n",
    "2.In terms of outliers, it is more robust.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "1.We have to apply gradient descent to the graph of MAE to make it differentiable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3d000-1be8-415d-9093-faddb27616be",
   "metadata": {},
   "source": [
    "MSE\n",
    "\n",
    "Advantages\n",
    "\n",
    "Unlike Mean Absolute Error, the graph of Mean Squared Error is differentiable, hence we can easily use it as a loss function.\n",
    "\n",
    "Disadvantages The value of MSE is in the squared unit of the output. This makes it harder to interpret loss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92394a10-4353-4b8f-8ec2-fcf22a797f0d",
   "metadata": {},
   "source": [
    "RMSE\n",
    "\n",
    "Advantages\n",
    "\n",
    "When it comes to outliers, RMSE is more sensitive than MAE, i.e., it penalizes the presence of outliers by producing large errors. This makes it useful when large errors are undesirable.\n",
    "\n",
    "Makes it easier to compute gradient when used as a loss function.\n",
    "\n",
    "Disadvantages\n",
    "\n",
    "When compared to MAE, it is less robust than other outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79893e10-e068-4f44-9517-a36a29662ee8",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is it more appropriate to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83b5335-4463-4a5a-850d-1cf6a177cbc4",
   "metadata": {},
   "source": [
    "Lasso regression stands for Least Absolute Shrinkage and Selection Operator. It adds penalty term to the cost function. This term is the absolute sum of the coefficients. As the value of coefficients increases from 0 this term penalizes, cause model, to decrease the value of coefficients in order to reduce loss.The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96ed2f7-16a8-46ae-97cf-33856593c744",
   "metadata": {},
   "source": [
    "It is more appropriate to use for feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb018273-76c4-4d71-b5ea-4ce9ffdb4d1f",
   "metadata": {},
   "source": [
    "## Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an example to illustrate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1edd734-d451-4eb7-b3da-cb2e2db27e4d",
   "metadata": {},
   "source": [
    "Regularization is a technique that adds information to a model to prevent the occurrence of overfitting. It is a type of regression that minimizes the coefficient estimates to zero to reduce the capacity (size) of a model. In this context, the reduction of the capacity of a model involves the removal of extra weights.\n",
    "\n",
    "Regularization removes extra weights from the selected features and redistributes the weights evenly. This means that regularization discourages the learning of a model of both high complexity and flexibility. A highly flexible model is one that possesses the freedom to fit as many data points as possible.\n",
    "\n",
    "Furthermore, in this context, we may judge the complexity of a predictive model by the number of features it possesses. A model with a lot of features to learn from is at a greater risk of overfitting. By discouraging the learning of (or use of) highly complex and flexible models, the risk of overfitting is lowered.\n",
    "\n",
    "Let’s use a linear regression equation to explain regularization further.\n",
    "\n",
    "Y=β0+β1X1+β2X2+…+βpXp\n",
    "Y\n",
    " represents the value that is to be predicted. βi\n",
    " stands for the regressor coefficient estimates for the corresponding predictor Xi\n",
    ". And, Xi\n",
    " represents the weights or magnitudes assigned to various predictors (independent variables). Here, i represents any value greater than or equal to 0, and less than p.\n",
    "\n",
    "A loss function is involved in the fitting process. It is computed as the difference between the actual and predicted output from a model. A loss function provides a means of assessing how well an algorithm models given data. It is used to minimize the error, in turn optimizing the weights. In this context, the loss function is referred to as the residual sum of squares (RSS).\n",
    "\n",
    "Below is the equation for the loss function.\n",
    "\n",
    "RSS=∑i=1n(yi–β0−∑j=1pβjxij)2\n",
    "Based on the training data, the loss function will adjust the coefficients. If the presence of noise or outliers is found in the training data, the approximated coefficients will not generalize well to the unseen data. Regularization comes into play and shrinks the learned estimates towards zero.\n",
    "\n",
    "In other words, it tunes the loss function by adding a penalty term, that prevents excessive fluctuation of the coefficients. Thereby, reducing the chances of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b150aec1-e939-447d-bbee-6aef7f4f7174",
   "metadata": {},
   "source": [
    "## Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best choice for regression analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9b010-cb11-4b97-88d2-ecee08659f18",
   "metadata": {},
   "source": [
    "Limitation of Ridge Regression: Ridge regression decreases the complexity of a model but does not reduce the number of variables since it never leads to a coefficient been zero rather only minimizes it. Hence, this model is not good for feature reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152b143c-0ef9-451d-b6a5-284fb48bace5",
   "metadata": {},
   "source": [
    "Limitation of Lasso Regression:\n",
    "Lasso sometimes struggles with some types of data. If the number of predictors (p) is greater than the number of observations (n), Lasso will pick at most n predictors as non-zero, even if all predictors are relevant (or may be used in the test set).\n",
    "\n",
    "If there are two or more highly collinear variables then LASSO regression select one of them randomly which is not good for the interpretation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55643b23-b663-47ab-b0b5-a1acb3ffc409",
   "metadata": {},
   "source": [
    "Elastic Net is proved to better it combines the regularization of both lasso and Ridge. The advantage of that it does not easily eliminate the high collinearity coefficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb32304f-01a1-4e69-958a-5316b1cd91d3",
   "metadata": {},
   "source": [
    "### Q9. You are comparing the performance of two regression models using different evaluation metrics.Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the betterperformer, and why? Are there any limitations to your choice of metric?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afedd5b7-79b7-4dd3-a68a-2be200942615",
   "metadata": {},
   "source": [
    "We want the value of RMSE to be as low as possible, as lower the RMSE value is, the better the model is with its predictions. A Higher RMSE indicates that there are large deviations between the predicted and actual value.\n",
    "\n",
    "A small MAE suggests the model is great at prediction, while a large MAE suggests that your model may have trouble in certain areas. MAE of 0 means that your model is a perfect predictor of the outputs.\n",
    "\n",
    "Model B is the better performer.\n",
    "\n",
    "MAE is not differentiable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc10116-472b-406e-9273-cb239f780836",
   "metadata": {},
   "source": [
    "### Q10. You are comparing the performance of two regularized linear models using different types ofregularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model Buses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the better performer, and why? Are there any trade-offs or limitations to your choice of regularization method?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da4f33-9e61-4d95-878a-0bc94e1dcb7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
